Feature Engineering
=

Before, you built a logistic regression system from scratch and tested it on
how well it could predict if an answer was correct.

You're going to continue to improve the accuracy of such a system by creating
new features.

You will improve the classification by extracting useful information from
the guesses and generate better features for input into the *pytorch logistic
regression* classifier to do a better job of selecting whether a guess to a
question is correct.

NOTE: Because the goal of this assignment is feature engineering, not
classification algorithms, you may not change the underlying algorithm. You
can change add to the guessed answers (e.g., to create a new feature), but you
may not swap out the class that's generating classes nor can you change the
classifier.

This assignment is structured in a way that approximates how classification
works in the real world: features are typically underspecified (or not
specified at all). You, the data digger, have to articulate the features you
need. You then compete against others to provide useful predictions.

It may seem straightforward, but do not start this at the last minute. There
are often many things that go wrong in testing out features, and you'll want
to make sure your features work well once you've found them.

How to add a feature?
-

0.  First, get an idea of what you want to do.  After training the classifier,
look at your predictions on
the dev set and see where they're going wrong.

1.  To add a feature, you need to create a new subclass of the Feature class.
This is important so that you can try out different features by turning them
on and off with the command line.

2.  Add code to instantiate the feature.

3.  (Optionally) Change the API to feed in more information into the feature
generation process.  This would be necessary to capture temporal dynamics or   

To walk you through the process, let's create a new feature that encodes how
often the guess appeared in the training set.  The first step is to define the
class.


Then the class needs to be loaded.  This happens in params.py.  Now you can
add the feature name to the command line to turn it on.

Before we try it out, we need to know what our baseline is.  So let's see how
it did *without* that feature.

So let's try it with the feature turned on.

Okay, so that helped!

Hmm, looking at the features, some of these values were really big.  This
might be giving too much weight to really frequent answers.

Let's try turning the raw count into a log and see if that helps.

What Can You Do?
-

You can:
* Add features (e.g., to tfidf_guesser.py)
* Change feature representations (e.g., in lr_pytorch.py)
* Exclude training data 
* Add training data

What Can't You Do?
-
Change the static guesses or use a different classifier (buzzer in this lingo).

How to start
-
1. Remind yourself how to run the pytorch logistic regression (lr_pytorch.py)
2. Add a simple feature to the training data generated by tfidf_guesser.py 
3. See if it increases the accuracy on held-out data when you run pytorch logistic regression (lr_pytorch.py) or on the leaderboard
4. Rinse and repeat!


Accuracy (15+ points)
------------------------------

15 points of your score will be generated from your performance on the
the classification competition on the leaderboard.  The performance will be
evaluated on accuracy on a held-out test set.

You should be able to significantly
improve on the baseline system.  If you can
do much better than your peers, you can earn extra credit (up to 10 points).

Analysis (10 Points)
--------------

The job of the written portion of the homework is to convince the grader that:
* Your new features work
* You understand what the new features are doing
* You had a clear methodology for incorporating the new features

Make sure that you have examples and quantitative evidence that your
features are working well, and include the metrics you chose to measure your system's performance. Be sure to explain how used the data
(e.g., did you have a development set?) and how you inspected the
results.

A sure way of getting a low grade is simply listing what you tried and
reporting the corresponding metrics for each attempt.  You are expected to pay more
attention to what is going on with the data and take a data-driven
approach to feature engineering.

How to Turn in Your System
-
* ``features.py``: This file includes an implementation of your TfidfGuesser, but more importantly, the function ``write_guess_json`` that generates the files for your logistic regression model.
* ``buzzer.py``: This file includes your implementation of GuessDataset and SimpleLogreg classes that we'll use.
* **Custom Training Data** (If you used additional training data beyond the Wikipedia pages, upload that as well
    * (OR) If either any of your files are >100MB, please submit a shell script named ``gather_resources.sh`` that will retrieve one or both of the files above programatically from a public location (i.e a public S3 bucket).
* ``analysis.pdf``: Your **PDF** file containing your feature engineering analysis.

Turn in the above files as usual via Gradescope, where we'll be using the
leaderboard as before.  However, the position on the leaderboard will count
for more of your grade.

FAQ
-----------------

*Q:* Can I modify buzzer.py so that I can use the history of guesses in a
 question?

*A:* Yes.

*Q:* Can I use the <INSERT NAME HERE> package?

*A:* Clear it first on Piazza.  We'll provide spacy and nltk for sure.  We
 won't allow packages that require internet access (e.g., wikipedia).  We
 don't have anything against Wikipedia, but we don't want to get our IP
 address banned.

*Q:* Sometimes the guess is correct but it isn't counted that way.

*A:* Yes, and we'll cover this in more detail later in the course.  For now,
 this is something we'll have to live with.

